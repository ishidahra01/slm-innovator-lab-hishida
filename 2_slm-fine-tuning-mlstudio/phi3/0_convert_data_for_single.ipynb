{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "module_path = \"../../0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert multi-turn conversation to single-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"../../1_synthetic-qa-generation/seed/dataset/azure-ai-services-openai-en-46-76-oai.jsonl\"\n",
    "output_file = \"../../1_synthetic-qa-generation/seed/dataset/azure-ai-services-openai-en-46-76-oai-single.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding='utf-8-sig') as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        messages = data.get(\"messages\", [])\n",
    "        \n",
    "        # systemメッセージが先頭に来ていることを前提\n",
    "        # messages例：\n",
    "        # [{\"role\": \"system\", \"content\": ...},\n",
    "        #  {\"role\": \"user\", \"content\": ...},\n",
    "        #  {\"role\": \"assistant\", \"content\": ...},\n",
    "        #  {\"role\": \"user\", \"content\": ...},\n",
    "        #  {\"role\": \"assistant\", \"content\": ...}]\n",
    "        \n",
    "        # 最初に出てくるuser→assistantペアのみを抽出する\n",
    "        # systemは残したい場合は残す（ここでは残す想定）\n",
    "        \n",
    "        filtered_messages = []\n",
    "        # systemメッセージをそのまま取り込む（必須でなければ省略可）\n",
    "        if messages and messages[0][\"role\"] == \"system\":\n",
    "            filtered_messages.append(messages[0])\n",
    "            start_index = 1\n",
    "        else:\n",
    "            start_index = 0\n",
    "        \n",
    "        # 最初のuserメッセージと、その直後のassistantメッセージのみ抽出\n",
    "        # userとassistantを1ペア見つけたらループを抜ける\n",
    "        user_found = False\n",
    "        for i in range(start_index, len(messages)):\n",
    "            if messages[i][\"role\"] == \"user\" and not user_found:\n",
    "                filtered_messages.append(messages[i])\n",
    "                user_found = True\n",
    "            elif messages[i][\"role\"] == \"assistant\" and user_found:\n",
    "                filtered_messages.append(messages[i])\n",
    "                # 1ペアのみ欲しいのでbreak\n",
    "                break\n",
    "        \n",
    "        # filtered_messagesが3件（system,user,assistant）になるのが理想\n",
    "        # user->assistantペアのみでもよければsystemを初めからスキップする\n",
    "        \n",
    "        if len(filtered_messages) >= 2:\n",
    "            # 新たなJSONL形式で出力\n",
    "            json.dump({\"messages\": filtered_messages}, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_bom(file_path):\n",
    "    \"\"\"\n",
    "    ファイルがBOMを含んでいるかをチェックする関数。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): チェックするファイルのパス。\n",
    "\n",
    "    Returns:\n",
    "        bool: BOMを含んでいる場合はTrue、そうでない場合はFalse。\n",
    "    \"\"\"\n",
    "    BOM = b'\\xef\\xbb\\xbf'  # UTF-8 BOMのバイト列\n",
    "    with open(file_path, 'rb') as file:\n",
    "        first_bytes = file.read(3)\n",
    "    return first_bytes == BOM\n",
    "\n",
    "# 使用例\n",
    "# file_path = input_file\n",
    "file_path = \"dataset/train.jsonl\"\n",
    "if has_bom(file_path):\n",
    "    print(f\"The file '{file_path}' contains a BOM.\")\n",
    "else:\n",
    "    print(f\"The file '{file_path}' does not contain a BOM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def contains_special_characters(file_path):\n",
    "    \"\"\"\n",
    "    ファイルに特殊文字（制御文字や非印刷可能文字）が含まれているか確認する関数。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): チェックするファイルのパス。\n",
    "\n",
    "    Returns:\n",
    "        list: 特殊文字が含まれる行番号とその内容をリストで返す。\n",
    "    \"\"\"\n",
    "    # 許容される文字（印刷可能なASCII文字と一般的な制御文字: \\n, \\t, \\r）\n",
    "    allowed_chars = set(string.printable) - set(string.whitespace) | {'\\n', '\\t', '\\r'}\n",
    "    \n",
    "    special_char_lines = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            for char in line:\n",
    "                if char not in allowed_chars:\n",
    "                    special_char_lines.append((line_number, line.strip()))\n",
    "                    break  # その行に1つでも特殊文字があれば次の行へ進む\n",
    "\n",
    "    return special_char_lines\n",
    "\n",
    "\n",
    "# 使用例\n",
    "file_path = \"dataset/train.jsonl\"\n",
    "special_chars = contains_special_characters(file_path)\n",
    "if special_chars:\n",
    "    print(\"Special characters detected in the following lines:\")\n",
    "    for line_number, content in special_chars:\n",
    "        print(f\"Line {line_number}: {content}\")\n",
    "else:\n",
    "    print(\"No special characters found in the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def check_encoding_issues(file_path):\n",
    "    \"\"\"\n",
    "    LLMのFine-tuningに影響を与えるエンコード問題をチェックする関数。\n",
    "\n",
    "    チェック内容:\n",
    "    1. BOMの有無\n",
    "    2. 不正または非標準のUnicode文字（制御文字など）\n",
    "\n",
    "    Args:\n",
    "        file_path (str): チェックするファイルのパス。\n",
    "\n",
    "    Returns:\n",
    "        dict: BOMの有無とエンコード問題が含まれる行情報を含む辞書。\n",
    "    \"\"\"\n",
    "    BOM = b'\\xef\\xbb\\xbf'\n",
    "    results = {\"has_bom\": False, \"encoding_issues\": []}\n",
    "    \n",
    "    # BOMチェック\n",
    "    with open(file_path, 'rb') as file:\n",
    "        first_bytes = file.read(3)\n",
    "    results[\"has_bom\"] = (first_bytes == BOM)\n",
    "\n",
    "    # 不正なUnicode文字のチェック\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            for char in line:\n",
    "                # Unicode制御文字（スペースや改行以外）を検出\n",
    "                if unicodedata.category(char).startswith(\"C\") and char not in {'\\n', '\\t', '\\r'}:\n",
    "                    results[\"encoding_issues\"].append((line_number, line.strip()))\n",
    "                    break  # 問題がある行を1つだけ記録して次の行に進む\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# 使用例\n",
    "file_path = \"dataset/train.jsonl\"\n",
    "check_results = check_encoding_issues(file_path)\n",
    "\n",
    "if check_results[\"has_bom\"]:\n",
    "    print(\"The file contains a BOM, which may affect fine-tuning.\")\n",
    "\n",
    "if check_results[\"encoding_issues\"]:\n",
    "    print(\"Encoding issues detected in the following lines:\")\n",
    "    for line_number, content in check_results[\"encoding_issues\"]:\n",
    "        print(f\"Line {line_number}: {content}\")\n",
    "else:\n",
    "    print(\"No encoding issues found in the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bom(file_path, output_path=None):\n",
    "    \"\"\"\n",
    "    ファイルからBOMを排除する関数。\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 元のファイルのパス。\n",
    "        output_path (str, optional): BOMを削除したファイルの保存先。\n",
    "                                     指定しない場合、元のファイルを上書き。\n",
    "\n",
    "    Returns:\n",
    "        bool: BOMが削除された場合はTrue、元々含まれていない場合はFalse。\n",
    "    \"\"\"\n",
    "    BOM = b'\\xef\\xbb\\xbf'  # UTF-8のBOM\n",
    "    with open(file_path, 'rb') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # BOMが含まれているかチェック\n",
    "    if content.startswith(BOM):\n",
    "        content = content[len(BOM):]  # BOMを除去\n",
    "        if output_path is None:  # 上書き保存\n",
    "            output_path = file_path\n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(content)\n",
    "        print(f\"BOM removed from '{file_path}' and saved to '{output_path}'.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"No BOM found in '{file_path}'.\")\n",
    "        return False\n",
    "\n",
    "# 使用例\n",
    "file_path = \"../../1_synthetic-qa-generation/seed/dataset/azure-ai-services-openai-pages-1-to-77-oai.jsonl\"\n",
    "output_path = \"../../1_synthetic-qa-generation/seed/dataset/azure-ai-services-openai-pages-1-to-77-oai-no-bow.jsonl\"\n",
    "remove_bom(file_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
